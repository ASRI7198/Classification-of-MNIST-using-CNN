{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "260dad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gzip, numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c5809a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "nb_epochs = 100\n",
    "eta = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c03d032f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_21808\\449228702.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ((data_train, label_train), (data_test, label_test)) = torch.load(gzip.open('mnist.pkl.gz'))\n"
     ]
    }
   ],
   "source": [
    "((data_train, label_train), (data_test, label_test)) = torch.load(gzip.open('mnist.pkl.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f23984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_validation, label_train, label_validation = train_test_split(data_train, label_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01f323fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(data_train,label_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(data_test,label_test)\n",
    "valid_dataset = torch.utils.data.TensorDataset(data_validation,label_validation)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5765e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        \n",
    "        self.Conv1 = nn.Conv2d(1,32,kernel_size=(3,3),stride=1,padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.Conv2 = nn.Conv2d(32,64,kernel_size=(3,3),stride=1,padding=1)\n",
    "        self.Maxpooling = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        \n",
    "        self.fc1 = nn.Linear(64*7*7,128)\n",
    "        self.fc2 = nn.Linear(128,10)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        out = self.Conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.Maxpooling(out)\n",
    "        out = self.Conv2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.Maxpooling(out)\n",
    "        \n",
    "        x = torch.flatten(out, 1)\n",
    "        \n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b3f7b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4377571",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2f32ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "p = 5  \n",
    "t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6951a538",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Époch 1/100, Loss = 0.0000\n",
      "Epoch 1, Validation Loss: 0.0407\n",
      "Époch 2/100, Loss = 0.0000\n",
      "Epoch 2, Validation Loss: 0.0336\n",
      "Époch 3/100, Loss = 0.0218\n",
      "Epoch 3, Validation Loss: 0.0335\n",
      "Époch 4/100, Loss = 0.0000\n",
      "Epoch 4, Validation Loss: 0.0347\n",
      "Époch 5/100, Loss = 0.4438\n",
      "Epoch 5, Validation Loss: 0.0325\n",
      "Époch 6/100, Loss = 0.0028\n",
      "Epoch 6, Validation Loss: 0.0343\n",
      "Époch 7/100, Loss = 0.0001\n",
      "Epoch 7, Validation Loss: 0.0291\n",
      "Époch 8/100, Loss = 0.4802\n",
      "Epoch 8, Validation Loss: 0.0308\n",
      "Époch 9/100, Loss = 0.0574\n",
      "Epoch 9, Validation Loss: 0.0306\n",
      "Époch 10/100, Loss = 0.0020\n",
      "Epoch 10, Validation Loss: 0.0279\n",
      "Époch 11/100, Loss = 0.0000\n",
      "Epoch 11, Validation Loss: 0.0284\n",
      "Époch 12/100, Loss = 0.0001\n",
      "Epoch 12, Validation Loss: 0.0297\n",
      "Époch 13/100, Loss = 0.0000\n",
      "Epoch 13, Validation Loss: 0.0292\n",
      "Époch 14/100, Loss = 0.0064\n",
      "Epoch 14, Validation Loss: 0.0264\n",
      "Époch 15/100, Loss = 1.6856\n",
      "Epoch 15, Validation Loss: 0.0279\n",
      "Époch 16/100, Loss = 0.0000\n",
      "Epoch 16, Validation Loss: 0.0261\n",
      "Époch 17/100, Loss = 0.0011\n",
      "Epoch 17, Validation Loss: 0.0262\n",
      "Époch 18/100, Loss = 0.0173\n",
      "Epoch 18, Validation Loss: 0.0266\n",
      "Époch 19/100, Loss = 0.0002\n",
      "Epoch 19, Validation Loss: 0.0303\n",
      "Époch 20/100, Loss = 0.0021\n",
      "Epoch 20, Validation Loss: 0.0264\n",
      "Époch 21/100, Loss = 0.0393\n",
      "Epoch 21, Validation Loss: 0.0259\n",
      "Époch 22/100, Loss = 0.0001\n",
      "Epoch 22, Validation Loss: 0.0277\n",
      "Époch 23/100, Loss = 0.0019\n",
      "Epoch 23, Validation Loss: 0.0269\n",
      "Époch 24/100, Loss = 0.0000\n",
      "Epoch 24, Validation Loss: 0.0266\n",
      "Époch 25/100, Loss = 0.0004\n",
      "Epoch 25, Validation Loss: 0.0241\n",
      "Époch 26/100, Loss = 0.0000\n",
      "Epoch 26, Validation Loss: 0.0256\n",
      "Époch 27/100, Loss = 0.0000\n",
      "Epoch 27, Validation Loss: 0.0276\n",
      "Époch 28/100, Loss = 0.0000\n",
      "Epoch 28, Validation Loss: 0.0248\n",
      "Époch 29/100, Loss = 0.0102\n",
      "Epoch 29, Validation Loss: 0.0265\n",
      "Époch 30/100, Loss = 0.0045\n",
      "Epoch 30, Validation Loss: 0.0239\n",
      "Époch 31/100, Loss = 0.0000\n",
      "Epoch 31, Validation Loss: 0.0234\n",
      "Époch 32/100, Loss = 0.0000\n",
      "Epoch 32, Validation Loss: 0.0228\n",
      "Époch 33/100, Loss = 0.0008\n",
      "Epoch 33, Validation Loss: 0.0239\n",
      "Époch 34/100, Loss = 0.0040\n",
      "Epoch 34, Validation Loss: 0.0226\n",
      "Époch 35/100, Loss = 0.1428\n",
      "Epoch 35, Validation Loss: 0.0242\n",
      "Époch 36/100, Loss = 0.0006\n",
      "Epoch 36, Validation Loss: 0.0229\n",
      "Époch 37/100, Loss = 0.0005\n",
      "Epoch 37, Validation Loss: 0.0244\n",
      "Époch 38/100, Loss = 0.0000\n",
      "Epoch 38, Validation Loss: 0.0237\n",
      "Époch 39/100, Loss = 0.0000\n",
      "Epoch 39, Validation Loss: 0.0219\n",
      "Époch 40/100, Loss = 0.0103\n",
      "Epoch 40, Validation Loss: 0.0215\n",
      "Époch 41/100, Loss = 0.0000\n",
      "Epoch 41, Validation Loss: 0.0251\n",
      "Époch 42/100, Loss = 0.0001\n",
      "Epoch 42, Validation Loss: 0.0245\n",
      "Époch 43/100, Loss = 0.0006\n",
      "Epoch 43, Validation Loss: 0.0220\n",
      "Époch 44/100, Loss = 0.0005\n",
      "Epoch 44, Validation Loss: 0.0208\n",
      "Époch 45/100, Loss = 0.0000\n",
      "Epoch 45, Validation Loss: 0.0224\n",
      "Époch 46/100, Loss = 0.0000\n",
      "Epoch 46, Validation Loss: 0.0215\n",
      "Époch 47/100, Loss = 0.0005\n",
      "Epoch 47, Validation Loss: 0.0211\n",
      "Époch 48/100, Loss = 0.0000\n",
      "Epoch 48, Validation Loss: 0.0221\n",
      "Époch 49/100, Loss = -0.0000\n",
      "Epoch 49, Validation Loss: 0.0203\n",
      "Époch 50/100, Loss = 0.0001\n",
      "Epoch 50, Validation Loss: 0.0223\n",
      "Époch 51/100, Loss = 0.0000\n",
      "Epoch 51, Validation Loss: 0.0205\n",
      "Époch 52/100, Loss = 0.0000\n",
      "Epoch 52, Validation Loss: 0.0211\n",
      "Époch 53/100, Loss = 0.0000\n",
      "Epoch 53, Validation Loss: 0.0212\n",
      "Époch 54/100, Loss = -0.0000\n",
      "Epoch 54, Validation Loss: 0.0204\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nb_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for images ,labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        # images.shape ==> torch.Size([5, 784]) alors : 784 = 28*28 \n",
    "        images = images.view(-1, 1, 28, 28)\n",
    "        outputs = model(images)\n",
    "        loss=criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Époch {epoch+1}/{nb_epochs}, Loss = {loss.item():.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.view(-1, 1, 28, 28)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(valid_loader)\n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        t = 0\n",
    "    else:\n",
    "        t += 1\n",
    "    \n",
    "    if t >= p:\n",
    "        print(\"Early stopping!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb2c057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "acc=0\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, 1, 28, 28)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        label_multiclas = torch.argmax(labels, dim=1)\n",
    "        all_labels.extend(label_multiclas.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ce9cf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds, average='macro')\n",
    "recall = recall_score(all_labels, all_preds, average='macro')\n",
    "f1 = f1_score(all_labels, all_preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c0b7e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.49%\n",
      "Precision: 0.99\n",
      "Recall: 0.99\n",
      "F1-Score: 0.99\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4627f490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0189763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
