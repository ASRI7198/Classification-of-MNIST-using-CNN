{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7b145c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, numpy, torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c609a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "nb_epochs = 100\n",
    "eta = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b99a325",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_32808\\1883767992.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ((data_train, label_train), (data_test, label_test)) = torch.load(gzip.open('mnist.pkl.gz'))\n"
     ]
    }
   ],
   "source": [
    "# Chargement des données\n",
    "((data_train, label_train), (data_test, label_test)) = torch.load(gzip.open('mnist.pkl.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97f2dabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_validation, label_train, label_validation = train_test_split(data_train, label_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "868f7044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on crée les lecteurs de données\n",
    "train_dataset = torch.utils.data.TensorDataset(data_train,label_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(data_test,label_test)\n",
    "valid_dataset = torch.utils.data.TensorDataset(data_validation,label_validation)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f2d91b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size, hidden1_size, hidden2_size, output_size):\n",
    "        super(DeepNetwork,self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size,hidden1_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.hidden2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.output = nn.Linear(hidden2_size,output_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.hidden1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.hidden2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.output(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db5074a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden1_size = 150\n",
    "hidden2_size = 150\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebd9d39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepNetwork(input_size, hidden1_size, hidden2_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "021f903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f6d007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e930d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "p = 5  \n",
    "t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "326633a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Époch 1/100, Loss = 0.3824\n",
      "Epoch 1, Validation Loss: 0.2509\n",
      "Époch 2/100, Loss = 0.0120\n",
      "Epoch 2, Validation Loss: 0.1764\n",
      "Époch 3/100, Loss = 0.0594\n",
      "Epoch 3, Validation Loss: 0.1484\n",
      "Époch 4/100, Loss = 0.0191\n",
      "Epoch 4, Validation Loss: 0.1302\n",
      "Époch 5/100, Loss = 0.1376\n",
      "Epoch 5, Validation Loss: 0.1165\n",
      "Époch 6/100, Loss = 0.1795\n",
      "Epoch 6, Validation Loss: 0.1125\n",
      "Époch 7/100, Loss = 0.0256\n",
      "Epoch 7, Validation Loss: 0.1070\n",
      "Époch 8/100, Loss = 0.1492\n",
      "Epoch 8, Validation Loss: 0.1019\n",
      "Époch 9/100, Loss = 0.0124\n",
      "Epoch 9, Validation Loss: 0.0953\n",
      "Époch 10/100, Loss = 0.0092\n",
      "Epoch 10, Validation Loss: 0.0954\n",
      "Époch 11/100, Loss = 0.1075\n",
      "Epoch 11, Validation Loss: 0.0938\n",
      "Époch 12/100, Loss = 0.0018\n",
      "Epoch 12, Validation Loss: 0.0947\n",
      "Époch 13/100, Loss = 0.7098\n",
      "Epoch 13, Validation Loss: 0.0901\n",
      "Époch 14/100, Loss = 0.0143\n",
      "Epoch 14, Validation Loss: 0.0883\n",
      "Époch 15/100, Loss = 0.0068\n",
      "Epoch 15, Validation Loss: 0.0889\n",
      "Époch 16/100, Loss = 0.1296\n",
      "Epoch 16, Validation Loss: 0.0880\n",
      "Époch 17/100, Loss = 0.0003\n",
      "Epoch 17, Validation Loss: 0.0900\n",
      "Époch 18/100, Loss = 0.3153\n",
      "Epoch 18, Validation Loss: 0.0836\n",
      "Époch 19/100, Loss = 0.0008\n",
      "Epoch 19, Validation Loss: 0.0870\n",
      "Époch 20/100, Loss = 0.3330\n",
      "Epoch 20, Validation Loss: 0.0816\n",
      "Époch 21/100, Loss = 0.0461\n",
      "Epoch 21, Validation Loss: 0.0821\n",
      "Époch 22/100, Loss = 0.0047\n",
      "Epoch 22, Validation Loss: 0.0769\n",
      "Époch 23/100, Loss = 0.6607\n",
      "Epoch 23, Validation Loss: 0.0787\n",
      "Époch 24/100, Loss = 0.0001\n",
      "Epoch 24, Validation Loss: 0.0788\n",
      "Époch 25/100, Loss = 0.0002\n",
      "Epoch 25, Validation Loss: 0.0828\n",
      "Époch 26/100, Loss = 0.0045\n",
      "Epoch 26, Validation Loss: 0.0810\n",
      "Époch 27/100, Loss = 0.0061\n",
      "Epoch 27, Validation Loss: 0.0767\n",
      "Époch 28/100, Loss = 0.0036\n",
      "Epoch 28, Validation Loss: 0.0804\n",
      "Époch 29/100, Loss = 0.0601\n",
      "Epoch 29, Validation Loss: 0.0771\n",
      "Époch 30/100, Loss = 0.0003\n",
      "Epoch 30, Validation Loss: 0.0798\n",
      "Époch 31/100, Loss = 0.0061\n",
      "Epoch 31, Validation Loss: 0.0801\n",
      "Époch 32/100, Loss = 0.0018\n",
      "Epoch 32, Validation Loss: 0.0775\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nb_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for images ,labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss=criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Époch {epoch+1}/{nb_epochs}, Loss = {loss.item():.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(valid_loader)\n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        t = 0\n",
    "    else:\n",
    "        t += 1\n",
    "    \n",
    "    if t >= p:\n",
    "        print(\"Early stopping!\")\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adba7bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "acc=0\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        label_multiclas = torch.argmax(labels, dim=1)\n",
    "        all_labels.extend(label_multiclas.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dd18a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds, average='macro')\n",
    "recall = recall_score(all_labels, all_preds, average='macro')\n",
    "f1 = f1_score(all_labels, all_preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c235646d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.81%\n",
      "Precision: 0.98\n",
      "Recall: 0.98\n",
      "F1-Score: 0.98\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b38fe30f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deep Network : \\n\\nbatch_size = 5\\nnb_epochs = 100\\neta = 0.00001\\n\\ninput_size = 784\\nhidden1_size = 128\\nhidden2_size = 128\\noutput_size = 10\\n\\nTest Accuracy: 76.40285583496094%\\n\\niterartion 2 : \\n\\nbatch_size = 5\\nnb_epochs = 100\\neta = 0.00001\\n\\ninput_size = 784\\nhidden1_size = 60\\nhidden2_size = 60\\noutput_size = 10\\n\\nTest Accuracy: 77.44285583496094%\\n\\niterartion 3 : \\n\\nbatch_size = 5\\nnb_epochs = 100\\neta = 0.00001\\n\\ninput_size = 784\\nhidden1_size = 30\\nhidden2_size = 30\\noutput_size = 10\\n\\nTest Accuracy: 66.44285583496094%\\n\\niterartion 4 : \\n\\nbatch_size = 5\\nnb_epochs = 100\\neta = 0.01\\n\\ninput_size = 784\\nhidden1_size = 150\\nhidden2_size = 150\\noutput_size = 10\\n\\nTest Accuracy: Test Accuracy: 97.9142837524414%'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Deep Network : \n",
    "\n",
    "batch_size = 5\n",
    "nb_epochs = 100\n",
    "eta = 0.00001\n",
    "\n",
    "input_size = 784\n",
    "hidden1_size = 128\n",
    "hidden2_size = 128\n",
    "output_size = 10\n",
    "\n",
    "Test Accuracy: 76.40285583496094%\n",
    "\n",
    "iterartion 2 : \n",
    "\n",
    "batch_size = 5\n",
    "nb_epochs = 100\n",
    "eta = 0.00001\n",
    "\n",
    "input_size = 784\n",
    "hidden1_size = 60\n",
    "hidden2_size = 60\n",
    "output_size = 10\n",
    "\n",
    "Test Accuracy: 77.44285583496094%\n",
    "\n",
    "iterartion 3 : \n",
    "\n",
    "batch_size = 5\n",
    "nb_epochs = 100\n",
    "eta = 0.00001\n",
    "\n",
    "input_size = 784\n",
    "hidden1_size = 30\n",
    "hidden2_size = 30\n",
    "output_size = 10\n",
    "\n",
    "Test Accuracy: 66.44285583496094%\n",
    "\n",
    "iterartion 4 : \n",
    "\n",
    "batch_size = 5\n",
    "nb_epochs = 100\n",
    "eta = 0.01\n",
    "\n",
    "input_size = 784\n",
    "hidden1_size = 150\n",
    "hidden2_size = 150\n",
    "output_size = 10\n",
    "\n",
    "Test Accuracy: Test Accuracy: 97.9142837524414%'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e79fdf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
